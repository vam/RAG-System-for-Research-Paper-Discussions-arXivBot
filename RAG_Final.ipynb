{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCXjXyMLaKrN",
        "outputId": "11e32c9e-eb85-4b08-d6f6-41a4b36c3e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74KuZOI92nNZ"
      },
      "source": [
        "#Model Finetunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_av50Czzo-l",
        "outputId": "e3d89250-9b6d-4da9-8adb-27f157c35a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU bitsandbytes transformers datasets accelerate loralib einops xformers\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595,
          "referenced_widgets": [
            "f74566e8e1f94c748974aeda2f953b7e",
            "230bee7912ba45f2be6870f95405e8bc",
            "b81fe2fd75234e8a917493df22c7c642",
            "8b0e1fdb1a014ab595f144bd614c614e",
            "ebd9689b6ae14b22845bb74b52f2b417",
            "2b503d5341c3477ebcd7050830d61ae3",
            "87c20aa7daeb4b6e88b13394458ac28d",
            "983825bff1ad4d4bbfcebfeceaf5a8ca",
            "503a9ce91e3c4f91b105b5e6a8df92a0",
            "35d3a4e7f8c9449cad64a0b1cb2f50b4",
            "56a98224ee22451ea26155db097e4acc",
            "fa23bd7be0ab4015818ca3745014178d",
            "bbbe49f0a8b14c368a0406d96ae08c0a",
            "61158311fed74cf6bee73f43bf597414",
            "826785d8bb9c4839b02a0065382f09c3",
            "e4ab040b3ebb42039ad23ca371982b8c",
            "393edc2a190e4cbdbfd752212cc3034f",
            "8b163a4d802c4d29972d0a836dfa17e0",
            "35ff25959bf0432695dfef2a21df55f1",
            "b2cd4bf867fb4f99b7e9461eec5fb0fc",
            "f040f89dc41e404a9b4e3bbcfb45c4e1",
            "9d1ae519ceee44b4b080d31f913d6c91",
            "f525affafe9f408bb7ec1fa051bcd409",
            "67885ea9936647528e47234ac8848d5b",
            "746bedc5d22f4366918bee36d8cc3cc6",
            "36ef6e54ffce4e31891efb0f705a0293",
            "daed500dc2f64a2c89608f2fccbad2d4",
            "72f4695f9e774463975021a76a9ba674",
            "8cb4cec6bda843c7b608cba31f33911e",
            "4efb7f94ed954c22ad31f4f290693cfa",
            "98fbae4a777f474bbf8178de53816b89",
            "d2feba5d92494eddbecd0b7674f63ddc",
            "3675927419f84d19abb4d7940701aedd",
            "59bd6d9d9dde4836bd61f949f155184b",
            "b9dba0b068e44d7fbff6b4cd76e60a7c",
            "3c5d31dbe881460aa058387c59c00470",
            "5910f2ab99fe4a0c8261ac8f36bd9dc8",
            "54dd9d1a9ec44815a573f186bd52c657",
            "2decae2192b24652aeb0189fcb1b0b19",
            "c3d94a1681474c9294ee106e44c5dfd0",
            "7c118f12667642cfa4a62420253a571c",
            "6df9f1f3f36e499fb40b97f1db56dd25",
            "d877a95ecb6a4c60a6819595f7969665",
            "5a923f06b5dc4d8fa9ad7ab682f74101",
            "6e09e06e5f8a445ea6f98ca6e93a5113",
            "be8f08ce0e1d4b6189b41dfceedabb05",
            "c96ad7866055497da9ecad58478c2659",
            "6c0c68ac75e84f55bb9a8c5237ff4ad8",
            "478b9cdea2344fa98f9c198e609daa40",
            "5c4a509646e74878975a7d14a5739f3b",
            "2cf74dc206a743dfb2152471f581a266",
            "1fa1e21add7648ca8b39c17432538bc4",
            "c3f8c17e10dd4e92828ca41de6437ba9",
            "e8ce9c06c3974553bc386e8f6fead90d",
            "5a75492b336d4580920ea80bcd82f2f0",
            "715f962ee3544e55bf14938fa3ca964f",
            "df3bf04aef644f84810a4fb06df370cc",
            "b3b7849896cc40f88531ea5325ded7d3",
            "09e86cdf00f9446089cb5de38736663f",
            "77fbd11469a044d9a30e5851a5c60174",
            "3bf0ea94cbbc494ca143e832e3f85f2f",
            "dd6b4c53af8745f5b16b0cd0f7085251",
            "788c84d15c96477e8d73e216972201e9",
            "b96a20d62be947b48d448e55dd78f358",
            "a8ff1d9fe4c2469faee4340d473b7d93",
            "fbfb0bb479754586bcca3c28726c0d1a",
            "17b6a595013443a68d6df048dc341be2",
            "b489ad3d409f4d978299fca0a5d7f757",
            "a849181a687c44599fbd1cad65afe7d2",
            "c3922d68cd2448a3a5a7e3aff8c3ed5c",
            "e1c6aac0b66e4921aaf33119425794b2",
            "9af67e174a44448aa51ed4feeba2f9a1",
            "2f3f1e7a48c642f08b37281860625d6a",
            "d4f0adca9a7e4634909f81c52b762eb3",
            "3b12bc84eea64d01bc9ce551f48c5e3f",
            "cdde0ee446c244c79074455d220417e8",
            "aace19566df044bea3bc7c81a5b463aa",
            "77b7aa4fe0904c96b63fe0a58633df68",
            "474c5ca635a340148b2c2cf5aaa9ff5b",
            "27d74c4051134234a564e544ad042306",
            "5608afd17ecb41f4a096c270fe67acb4",
            "3da26494b6404dcc833057115b4d47e9",
            "dfac40f3dff2431bb9e8acd2607df893",
            "1a4839c17b9e4a9ba5bc205e60dd2954",
            "4adb50791ad04a919de6ab60110a37e8",
            "9c92df6df583494ebf30a1769b63fe41",
            "38830ea6f3cb4986987207a385db6420",
            "60b35902c8254ca491265bff94b6b1ae",
            "1eb5848db05c4f469cfbbeca16c49d74",
            "eebc6465fc8b4e07aeb3f22da3a625c1",
            "fab919a60ef34871a422a3c3c4e4b23f",
            "bccacf1399a34ebaa1d96c59786ee62f",
            "7231e99bbbef4b19991982ccb9430044",
            "1d09414cb2a7457ba727132b05c1739b",
            "8be7dfa3accb468d8dd1bf2586128139",
            "db194c0a3c4e490099e45dcb7a1688c4",
            "7bb626e713a942efbc1ef88c2fd2a8a7",
            "6420e633503d4320aff68608c2cba0c7",
            "764cbd9d251a4a3e9913c2f9d59858ec",
            "ce5b13d0a174478ebae2a77d05fff6f3",
            "5e7e814caa504da38f22e55332fe3791",
            "a9577a3214bc4d00af74f2c843d5f9d8",
            "f76ec78b834248d288f522b1eac40f64",
            "21d538873cc64d70a6d0c67de38d5c0a",
            "00fc93e355534d9da90f5b9be6267c9a",
            "f0c43a819f214544946ec2426dfdd375",
            "14b13bc85c61451b931e4884e406a1c8",
            "d60242f1d97442f4bf7a6bde2c7e062e",
            "b3b411a8ece24a7ea671ffa05bd25bda",
            "5fbe65b920dd4ece93772ddbe5d32517",
            "792bfc7db3d44fcf9d18a223e7b223c8",
            "c8974475d22d48f987670d40e878deb1",
            "2de9ff97e45244a6ac64381c4d167c1e",
            "c34eeae7870a4829889e86661ae7b59b",
            "c2fded161b7d4286a46025c97f5b5b98",
            "87aea2901a5f42b5bec5431ee8a46571",
            "7dc8601f6d6d4a5b9643b86bcb593211",
            "e3ea60876d814de798f888c37ae7c1c6",
            "6021c8501177454ca9448ed3d69df451",
            "68937352a0b9491bb950ef1ff589b021",
            "363d93b71c444271bc2af2de8d525657",
            "94324016ad1b46128e175648fd61c0f7",
            "53cdc483d99a45598d21d879968fa2e3",
            "a8b45dbb632942fb87f30106ce186eca",
            "300aaae1497b41979ec4d47b2dbba33a",
            "24fe55b14dd347608a235618c59e31a2",
            "4b5b2bc3c9b7411894759fd2c0e1d846",
            "aa1f96948a0f43eb97312d9ef7d61a52",
            "2f8f9ad9aed14b9a93fcd84a9fe9e50d",
            "3179505d833546a2af7b82a87a1af107",
            "99250a50f1ad4832957862834fce31b3",
            "98d46fe36e924be69f0118886f19e702"
          ]
        },
        "id": "r271gWV39ufd",
        "outputId": "5b7de6a3-ee30-47bd-a9ec-2992e8a4d678"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['load_4bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f74566e8e1f94c748974aeda2f953b7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa23bd7be0ab4015818ca3745014178d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
            "- configuration_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.tiiuae.falcon-7b.ec89142b67d748a1865ea4451372db8313ada0d8.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f525affafe9f408bb7ec1fa051bcd409",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
            "- modeling_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59bd6d9d9dde4836bd61f949f155184b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e09e06e5f8a445ea6f98ca6e93a5113",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "715f962ee3544e55bf14938fa3ca964f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17b6a595013443a68d6df048dc341be2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77b7aa4fe0904c96b63fe0a58633df68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1eb5848db05c4f469cfbbeca16c49d74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce5b13d0a174478ebae2a77d05fff6f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "792bfc7db3d44fcf9d18a223e7b223c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94324016ad1b46128e175648fd61c0f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_id = \"tiiuae/falcon-7b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    load_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model =AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCNoefXp9wjh",
        "outputId": "9c16a3c0-5705-4423-bee8-f43abf498064"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
          ]
        }
      ],
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9yoqn8o9yfq"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "09f7a27b0578445a915aa6980679dd58",
            "7f5809c7eaa84ea3809647d9555efdaa",
            "7c4ddc145d6846eb9e745d13139847bc",
            "c063f243ecdb429a91a42f27c71279a1",
            "cad8ff03361f4a9ba218ff253225dfa5",
            "b907543391f74c2096ed227b6c6e5847",
            "a6379c8574fe47279a03d23bd242decf",
            "cda74e55525f436c9c7b5c44964e49ae",
            "c09732861d1d4c5c956281d5ee94c123",
            "b3a5fc1a2e524a03ae0313ca07348327",
            "27e2b615db864006b46e8432235d7f60",
            "b5c98a9919bd4c7289f3247f69bcb866",
            "01b840b1113046ae83d9599a5ee88098",
            "8cc98f0d8f44435e8fa4e1bba6ac2ad4",
            "2ca8ed11628c4aa88ef63a8e0e22d30f",
            "acfc03427b964e15b998a0cdf3d1f83c",
            "d0f53586e1b940b1a80b2fe0fbf32599",
            "a24edc5e4fd24f869c288b44faece593",
            "db0e97ec82664d83956d9f976573a7ed",
            "d69377399ff843d682c599d51eb1d00b",
            "6f46ff549a4341979906736b2f210aa9",
            "6d93b3df1c824e6ebebab6a511139c60",
            "3c52ece9cec54dfea6c95e550c9c132d",
            "958c8bc2d7774c0b9af55560928262a6",
            "d1d5ba7a3f7445cc995f64cdfa31229f",
            "b40c9a76f2d347e98fcdb11764dafe46",
            "11c819207d1b47b0b72b9555da4acfa7",
            "cdfb8ea053de4be18bfbf51a4fd623b2",
            "78e627fb73834bb588f8be7bbde29636",
            "cc1b3bc7cfc84c91a0e8ea2aec353a70",
            "71cce464a84449dd984de42055b261ad",
            "9f7d5f24f94b4768bdd8c17b904e5983",
            "9e91b35b56c943c7bcd1759de8de236b"
          ]
        },
        "id": "IyfW40ttzpOo",
        "outputId": "abd12a9e-4db1-4ec1-a12f-01db3f8d0f6e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09f7a27b0578445a915aa6980679dd58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/547 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5c98a9919bd4c7289f3247f69bcb866",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)-00000-of-00001-d9a650f63b16925f.parquet:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c52ece9cec54dfea6c95e550c9c132d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/10861 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "<Human>: {data_point[\"instruction\"]} {data_point[\"input\"]}\n",
        "<AI>: {data_point[\"output\"]}\n",
        "    \"\"\".strip()\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    # Tokenize the generated prompt\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load your dataset\n",
        "dataset_name = 'amrachraf/arXiv-full-text-synthetic-instruct-tune'\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "\n",
        "\n",
        "# Now the dataset will be tokenized and ready for fine-tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogr33vZgvbIv",
        "outputId": "368e5ffc-e895-4341-8435-6c5a1e1f0cd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10861"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plWukKhzvaGL"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.select(range(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "441f0855a06546758dff6f78ebffdd48",
            "140019e71e0b4b8d9d9f995fdfc67f81",
            "bdbd42eb022c49cc96e62994312673de",
            "bbc6023976654b8ba9d15bba69a8e796",
            "7063370f63b04534ba5b507f25334963",
            "67f7834bcfec4909b7536445fb7aa728",
            "73a5e61b9a7a4260ac34bf413e66fb55",
            "758e17c7614044e3b041b7d1351d4352",
            "cff0b2be0b05478a8965d5f714d29039",
            "d6e30c6c313943d287b17e4568a3e26a",
            "7a36900e8d994b6fb17b19996084104b"
          ]
        },
        "id": "KgUGoNlpvUvF",
        "outputId": "fb71d033-e8a8-4338-88ee-e9a8d4167932"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "441f0855a06546758dff6f78ebffdd48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Shuffle and apply the tokenization function\n",
        "dataset = dataset.shuffle().map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7u6IWUg9hd4"
      },
      "outputs": [],
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    auto_find_batch_size=True,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    save_total_limit=4,\n",
        "    logging_steps=10,\n",
        "    output_dir=\"/content/\",\n",
        "    save_strategy='epoch',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mp6dOK7h95h2",
        "outputId": "59ce0610-f98d-44b1-c03a-10a8084d308e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241118_180850-77viu6lk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/raghavapurapuch-iit-bhilai/huggingface/runs/77viu6lk' target=\"_blank\">/content/</a></strong> to <a href='https://wandb.ai/raghavapurapuch-iit-bhilai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/raghavapurapuch-iit-bhilai/huggingface' target=\"_blank\">https://wandb.ai/raghavapurapuch-iit-bhilai/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/raghavapurapuch-iit-bhilai/huggingface/runs/77viu6lk' target=\"_blank\">https://wandb.ai/raghavapurapuch-iit-bhilai/huggingface/runs/77viu6lk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  3/250 00:57 < 3:55:01, 0.02 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='148' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [148/500 49:42 < 1:59:49, 0.05 it/s, Epoch 0.29/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.063200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.033400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.104800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.908600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.262100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.861800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.921500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.729300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.949700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.906600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.993400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 2:44:07, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.063200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.033400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.104800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.908600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.262100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.861800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.921500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.729300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.949700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.906600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.993400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.783700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.961600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.935700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.800300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.021100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.972500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.010900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.718100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.027600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.765600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.915000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.900600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.733500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.798600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.779100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.903700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.834600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.776400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.775400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.925200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.887600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.746700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.757100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.603700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.652200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.774400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.865100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.813000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.978900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.848300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.807400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.723600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.681900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.706000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.8702558784484864, metrics={'train_runtime': 9881.5387, 'train_samples_per_second': 0.051, 'train_steps_per_second': 0.051, 'total_flos': 1.78437419867328e+16, 'train_loss': 1.8702558784484864, 'epoch': 1.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETJwXBlFABH8"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/fine_tuned_falcon_7b_instruct_lora\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_44gjKYjduoE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuloGrWmEMdj"
      },
      "source": [
        "# RAG Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX9QlhFdEMue",
        "outputId": "f238fae5-7341-4d65-97e3-ecdd4ce5f11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.0/189.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.3.dev0 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.6/915.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU llama-index\n",
        "!pip install -q llama-index-readers-papers\n",
        "!pip install -q llama-index-llms-huggingface\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q langchain-community\n",
        "!pip install -q llama-index-readers-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqmJUDKjEYJY"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.papers import ArxivReader\n",
        "\n",
        "loader = ArxivReader()\n",
        "documents, abstracts = loader.load_papers_and_abstracts(\n",
        "    search_query=\"Super Resolution of Images\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEK3N8_nEicL"
      },
      "outputs": [],
      "source": [
        "# documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcMatxFwEsAe"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage\n",
        "from llama_index.core.node_parser import SimpleNodeParser,get_leaf_nodes, get_root_nodes\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core.response_synthesizers import ResponseMode\n",
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.retrievers import AutoMergingRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "parser = SimpleNodeParser()\n",
        "\n",
        "# Parse documents into nodes\n",
        "nodes = parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "48975a6bd08c4036bfecc53989226319",
            "55b6b870eb09471b85dcc8406abb4b45",
            "116f712fcf024766b980eb09cf2ce5a4",
            "9dce4fcc22054c3f8793583296d481e2",
            "c81b12c5b2054b10b7460b93e61192b6",
            "e1b18b13e26d4e09a1a474ebe313d865",
            "18e54fcf80df4893bb1d48b39f8eb392",
            "93a923d4297f4500b4f5127a9551ec6b",
            "d006d290177d4150b6f4b0cccce73ba2",
            "f7a92c944e6241cebe04c177a9aa0d4e",
            "2783ec961b29442fa726f6973fe8357a",
            "82810c90766b4cc0a10181673843ec3d",
            "1be56ca9005346c29b38bf8d75f25d52",
            "14282378c4c046cebae2fc0013491ed7",
            "08e6227416ac487e8d51624e0922082f",
            "ab74d5ad7bb54f4992b7f141ecc22e78",
            "4f51b4ac8425407099a58bc65b5dbc11",
            "071152e22cde4c93bf6082ffe097b023",
            "9b2d34c0119a462abc2eb680873214ab",
            "e4d87b5039de426b9489ccbd0bc05f77",
            "176d4e2de492407487454ccb705cab5e",
            "c4160416a5eb4f888353fdfba32314de",
            "e35eb44b993346f4ba8e65d77baa21fd",
            "549abcd9d57b446ba4f5c9455c2cb88b",
            "d3a821f1108a4f8bbfd9496ba50ae888",
            "32a3b4eb19574f909d8cceffb8681779",
            "fdd4e0114b274dec9cac29cd36d06f7b",
            "d9967e9bfb644489bdf619b8f86a6111",
            "ac6e3e89faa44dbdada2c78993021bae",
            "0decf2655ace465cb183e16e5b050577",
            "6a0a270a6ec84ded813a0bb702f922fb",
            "282382332a4341c1a5c62a49fc98a9aa",
            "de369dc753e34ad4bfab9a38582a4e8d",
            "0df1662093304458a56cdc85742c93b2",
            "ea3e76e38790474aae8af693b98125a4",
            "cfee0b7bd2ee46e08cc98393d350f44b",
            "0ccd3d44a47a464297d40a0cd232a384",
            "a9ba0cbaeaf34963bd3df08741a8f6fb",
            "80e3fa88029b46a18e078eea47cdd10c",
            "122e600a40214c768fd0e6543c6c55ba",
            "b4803535ed7d42d6a3a55044af5a3076",
            "6ac098009f844bbeac04db347b878df1",
            "9c5a457bd6f14212b68834c0eb9ad8b4",
            "0ed78a0c1c494e3aa3dec92040786718",
            "30fe6485762142139216d44cb014088f",
            "9c2a08483b914adcb9baf6e9e878c1fc",
            "68d602a9d52b4878a90e4b83eed19e2b",
            "069008fab8b046dbb053edca64c928f0",
            "9221e7ade6444d1ebfcef2db6362d12b",
            "21b9e90306c54f25a9aa9173e0d9ea1e",
            "2d4f1335b92c48cba6a9f658ea1d2d10",
            "422d9e08a5f04382b022c28ecbeec9a0",
            "7918c23a885543298713ae6cb09912df",
            "dd417463b5e847c3a51a1e4866426727",
            "fb4c25e809e84431bc5856a8ec6218ce",
            "99cc3bf2d82a45148add731804d16e76",
            "a50fb33413e641ed995401d1a55c554c",
            "b3438ee7cdda45cfb1aa84784a07bb6e",
            "738d9800680c4ea895b24808cdfd7299",
            "29258a0b3e444cbeb5ab5ef2468175bf",
            "433a18c9b00c4c869edab5d81234f0da",
            "329bfbfd04fa41ea9796a477d81b76bb",
            "dce4427361fb47fc90936d00e88067a5",
            "75fe448f5cf842328d01b4ef18d19696",
            "541153ac8dec43f4bd39c3c6ee29c367",
            "0949896abaa34afaad097ab0b8ac204d",
            "6ece39232e024dc3b12aeae882cbe1b5",
            "8dbaf765a521470faf1a4d082eadea15",
            "bf1572de6e87402c9c782ef6274f1d23",
            "63d4c35bcb2c4c3e93fb224150bfeeb1",
            "1bd1c2f294ed41488e81bb33c06f9f82",
            "13d47a49320e4af38b3f2d0c1413cb94",
            "af095c14639f4d159bd7e503bfcc0817",
            "7269a5d76ee94a539117469cfc2b5c81",
            "f01474285e8b42b2bbebd2770b0ae4eb",
            "169c9200019d46b5abac3bf161a4d852",
            "ffc3234686664118a2db926fc381078a",
            "ad6b58bef1bc402599f8050bf3800752",
            "3dbea787fe9348b1ae7c3250577685c6",
            "0b10f01bbc47469899cd062175da4c44",
            "8fe0c0b6d10543a4ba4dedeaaba325a9",
            "4d8e44f1c2794abab8b00e0b6d113e67",
            "87ce390f992c41228f7b9a59c175fcd7",
            "a0b186cbed514e54943c788254d44975",
            "c5b9c6ffec99429d9b595b7edbc9e656",
            "119437ab01c14107b2ccbb4c4ae92339",
            "7276e666e8be49d19f5666cdbf819ac8",
            "3bda17a7361b456f83f710a69fa0c4f6",
            "457b1bcf16d5467faf3f8bf1c88d37d4",
            "64b57b7df0c24b06a69649be58ef5dc8",
            "8d29b1ce1a964ae28969243841f8fc8b",
            "1fbbbb0b80eb4e2389494d6b0d3a44ef",
            "7f4a4cc6130441b4ab3aaf34a89ea8c9",
            "05c6d3775c5646caa1deebf53dab48eb",
            "e35f9e717cfc4f57b1db1399f4a961b3",
            "d6eda76073704a71941b7dadb18216d9",
            "12c0e0a2430940ac81a8a0ec9d56006b",
            "75c55e141d724f08a46081ce2cc1495d",
            "3827c537b13c43a0aba030c94f239a57",
            "1ddcdb07a8ca4e07b04531405b5bfd07",
            "4a48a197d21b4b96842c1d1734735a23",
            "4d86dae6c92c46bcb7fbe2f98eda861f",
            "4c6b09e32aa545aa96335186a9a35be4",
            "79118cc15d3e48dba4f2413b51596272",
            "08d459babcd34da48a699b8e88e45dc0",
            "4fc27c7365a44d40922841e479c4aae1",
            "3dc0db55b64d4c6fbdc46ef30d71db1c",
            "43f97e6ca7f14523bf64b243e489db29",
            "ee8d0a7f952d4cf194d5cfc340ffdf6a",
            "e91789ecdb68469f9a6d42c2a9886903",
            "483339c4c72b460ea8facc014dfe63eb",
            "637a2e6fff4e421f9f61117fb65427af",
            "222eea0fbba144a892edbf6dc14721a8",
            "86ba1f5d12d5459ab24ac71156e5b9d2",
            "1a4f37a1858e412cbf31f396c1bf7f4a",
            "2e4cce36a7c04e308def3607dc1a0543",
            "3999d136adb44bba8e5b7d44304e8c87",
            "dbfc951b8e1d496abe3f0a441ed3768f",
            "7e9b67c3204444bea8acddb067bef7c6",
            "2d79a923cafd4eac9d083fb80020e678",
            "6043a4d6184145d09562591cc98682b0"
          ]
        },
        "id": "mmUWP88UFPoE",
        "outputId": "e87f43bc-dd2f-4695-e8c7-16213b363055"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48975a6bd08c4036bfecc53989226319",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82810c90766b4cc0a10181673843ec3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e35eb44b993346f4ba8e65d77baa21fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df1662093304458a56cdc85742c93b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30fe6485762142139216d44cb014088f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99cc3bf2d82a45148add731804d16e76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ece39232e024dc3b12aeae882cbe1b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad6b58bef1bc402599f8050bf3800752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "457b1bcf16d5467faf3f8bf1c88d37d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ddcdb07a8ca4e07b04531405b5bfd07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "483339c4c72b460ea8facc014dfe63eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS9mlgzCFeEa"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "index.set_index_id(\"vector_index\")\n",
        "index.storage_context.persist(\"/content/drive/My Drive/RAGgers/storage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWL6QFtMF6ht"
      },
      "outputs": [],
      "source": [
        "# Rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"/content/drive/My Drive/RAGgers/storage\")\n",
        "# load index\n",
        "index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVFC87GIF_VV",
        "outputId": "df398e0a-3bb4-4dca-934b-e5e0047f2782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q llama-index-llms-llama-cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWtelzO9IXGC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499,
          "referenced_widgets": [
            "1dc6196cb4b9428dbcafab368a9d491d",
            "9b9e390539934141a44e02e6d8164a5b",
            "07eb6038efde438fa8b20deeba2b8d83",
            "55a29ca8d0054075bff2951c83b42357",
            "3ecfe59f88de475aa852ed1a8b3ef37c",
            "40e8dbf7fd8648929135012c155bccfb",
            "77663cfc5bc8455ea49b0a6647358bc0",
            "8da59aa1eddd4562bd29469894adb693",
            "4cdf83b63b15424fa0d2f80e848dd107",
            "430ef73618354931b56344dddb1ac606",
            "ee5a3089a70c44a19718f6600c8b7b6f",
            "284fef9ceb204b5289cc8e9502970950",
            "434b2bb0d1d949ab8774587d16ab7459",
            "305c209d39d1468584c10707f0d807a5",
            "c256b9f0a5aa4f558171575e108052dc",
            "d066da8983444897a53c8074a994fa47",
            "adcc1449c7294e17b83d70acfae23819",
            "cfbdaf77a6704bbf98623080b1b63f61",
            "9e26bf00b1864dcebbf4253a4e44c346",
            "9883301d555745d5be0431b67ff8d427",
            "e74c8160f11d4f9e8cc7556423127ca6",
            "474ac3101f0843e588f6ee3c6d31a240",
            "df13b00f3bcf46889a06bdc26949f8bd",
            "10b135612cb84e15acd0319b2cc19ddf",
            "8d0aa58e457f49d989f16ecac970810b",
            "9c710de22ab641b2a784ec2fc822436e",
            "06e6d452fd70498db20310758332a70a",
            "48a2c8c5b0ba4b5e932e4dce2cf4e958",
            "2047b4ce3dd14186822ccb5c996e7e19",
            "e39f567859184ef68fdba937c93edb4b",
            "529344dd5baf468288a59395b099c195",
            "30fe6012f7ff408dacb497bae786ab9a",
            "b13466f09a4e45e4992ac15ff7453355",
            "46da98a66db94f559cb7f0da05b7c244",
            "dd04cd0773c64270bcc90282ba2de11b",
            "b3de406ba61e49f2ab07b2daab4dc471",
            "a5aa0c3b059e4d018edebbc2c03918fa",
            "f0f7e6f1f6c64405b1dead07a6f623c8",
            "8e549d1e186447e2b8ae2362f7fead8b",
            "5d10a511e24a45378f7f46f63c926e38",
            "2374356cc66a47808c2e87885c20d481",
            "144355a4aa5c4e0880dd5f716fd38335",
            "92bed1249a324a679b9e8f4e64c02338",
            "3a25592b7b094bf3963cf231093cd144",
            "bfd61fc3da4d42769c2b8747461beaba",
            "90fbe30c16c043728738414be50197ee",
            "7f70f925dff9473abff6487bbbdeeae8",
            "d55f0beeaeed4ffabfe0c393a44f0cc9",
            "2bc235e04e5b41a081ee20840871bb0f",
            "6cfb0cb5c17e4499bde9fa9b87c7ea40",
            "920e0d9dc92f4dde80648a7c1cf18d56",
            "6354da35b6b14d779c0f5a1acee0458b",
            "c1339427344a4f208ac4c81959481a21",
            "7d3b2d44b67146029779c83f66bb4103",
            "34467e78cc6d41639366656b846bb744",
            "ec8412b5d3df487baf67b5959ffd8b7b",
            "ddf3dddb651f4043a577ded8238e1923",
            "17ade0cb6aed43559135c3d465271234",
            "7a43b07edb0f46dfaf055a439e58ddc6",
            "f4d16bb5da7b4342a9c11a1523fbff29",
            "6e26959c3f9c47edbb376cd27a6e3b99",
            "8ec1ec5275d1469c9f1bcda4a099846f",
            "7d7d011c15134d83be5776adb42e231a",
            "c95bc7c7e96742e2958fb61d71121640",
            "f3de56c3dff94be491e44ae6d0b60b27",
            "e160780659ca4c288430382128c94611",
            "9812f6eb68774852951f6c9b29393cfa",
            "9d9e81325f3742f3b6853c0078927dcc",
            "d486e6deeb1a4be2b19b5a12a1899697",
            "73a3f0c2c14f4e29b0f44f04f490302d",
            "014899ca09d545018a56f5bcecf7e8dc",
            "55e76ecee7414d499c80672e917fbc49",
            "b9685a1feb3742b7b5568a0a7fadef01",
            "6b4445f65e034059af34d0f711a44b9f",
            "f3ea678c3db643e78482abca2657c41f",
            "365b255122494cfbb57f49a504450701",
            "62523f1c44a14bdb87c25be8c0a59db2",
            "4d30af7431ec49929c18c7ff3bceacfa",
            "336aac76cfe24b1f9b342fb7d0d8a8d4",
            "4a51675d66d0403797e377c018d357a8",
            "3dc22f0bcba145c5999399fa5dcd6775",
            "65b3bbf574b343709e4c208dd6d69e3a",
            "f02b6b5a0e814fcaafd88247c5176a50",
            "d52ca5dde5b8440f9a4f2dc240c6f1b5",
            "dac9c41e549c41a9a65a861006c8dbf2",
            "179aedb28a2942ef9a566f2a7265b0ec",
            "6172c73aa7444d36a9962791423290a4",
            "aae86d6136584ab681391f3da40fded8",
            "3d058861baa347b291e156282ba7e99e",
            "c71a6e602a9349e68fb4bcc90f9e64b4",
            "b3c9fbe2d0484a27a17ecbfa784b0c49",
            "fa7a0955d804446aa509549e482d02b6",
            "a2e51425d32a4a56bac8320d6ad83d56",
            "52ed8f2702d04416b894f67161f29daa",
            "902e7a9965b34c918870a49c7dc0c316",
            "e73bc79de97b4bbe8376096d66df695c",
            "b1b3c112f4e7497d916bbb58a4189106",
            "4e4d78489969474682fa878e145a6cca",
            "49a6888004e24f749b6c7d27554ef2d7"
          ]
        },
        "id": "Y4aQ8G2jTe-L",
        "outputId": "d72b0edc-107e-42e3-ce99-d6c72e9241ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unused kwargs: ['load_4bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dc6196cb4b9428dbcafab368a9d491d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "284fef9ceb204b5289cc8e9502970950",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
            "- configuration_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.tiiuae.falcon-7b.ec89142b67d748a1865ea4451372db8313ada0d8.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df13b00f3bcf46889a06bdc26949f8bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
            "- modeling_falcon.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46da98a66db94f559cb7f0da05b7c244",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfd61fc3da4d42769c2b8747461beaba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec8412b5d3df487baf67b5959ffd8b7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9812f6eb68774852951f6c9b29393cfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d30af7431ec49929c18c7ff3bceacfa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d058861baa347b291e156282ba7e99e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/RAGgers/fine_tuned_falcon_7b_instruct_lora\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    load_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"tiiuae/falcon-7b\",  # Base Falcon model\n",
        "    device_map=\"auto\",  # Automatically map layers across available GPUs\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load fine-tuned LoRA model\n",
        "model = PeftModel.from_pretrained(base_model, model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYXCm7GUyG4z",
        "outputId": "6d4ca254-2ea2-4ace-d02e-67637f4e23a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in ArXivLLM has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from typing import Any\n",
        "\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "\n",
        "\n",
        "class ArXivLLM(CustomLLM):\n",
        "    num_output: int = 2048\n",
        "    model_name: str = \"ArXivLLM\"\n",
        "    model: Any = None\n",
        "    tokenizer: Any = None\n",
        "    context_window: int = 2048  # Define model's context window size\n",
        "\n",
        "    def __init__(self, model, tokenizer, num_output):\n",
        "        super(ArXivLLM, self).__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_output = num_output\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        \"\"\"Get LLM metadata.\"\"\"\n",
        "        return LLMMetadata(\n",
        "            num_output=self.num_output,\n",
        "            model_name=self.model_name,\n",
        "        )\n",
        "\n",
        "    def _get_available_context_size(self, prompt: str) -> int:\n",
        "        \"\"\"Calculate available context size based on the input prompt.\"\"\"\n",
        "        input_tokens = len(self.tokenizer(prompt)[\"input_ids\"])\n",
        "        available_tokens = self.context_window - input_tokens\n",
        "        return available_tokens\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
        "        # Calculate available context size\n",
        "        available_tokens = self._get_available_context_size(prompt)\n",
        "\n",
        "        # If available tokens are less than or equal to zero, raise an error\n",
        "        if available_tokens <= 0:\n",
        "            raise ValueError(\"Input exceeds the model's maximum context window.\")\n",
        "\n",
        "        # Adjust max_new_tokens to fit the available context size\n",
        "        max_new_tokens = min(self.num_output, available_tokens)\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(prompt, truncation=True, return_tensors=\"pt\", max_length=self.context_window)\n",
        "\n",
        "        # Generate output\n",
        "        outputs = self.model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=len(inputs.input_ids[0]) + max_new_tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=self.tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "        # Decode and return as text\n",
        "        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return CompletionResponse(text=output_text)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
        "        # Calculate available context size\n",
        "        available_tokens = self._get_available_context_size(prompt)\n",
        "\n",
        "        # If available tokens are less than or equal to zero, raise an error\n",
        "        if available_tokens <= 0:\n",
        "            raise ValueError(\"Input exceeds the model's maximum context window.\")\n",
        "\n",
        "        # Adjust max_new_tokens to fit the available context size\n",
        "        max_new_tokens = min(self.num_output, available_tokens)\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Generate output\n",
        "        outputs = self.model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=len(inputs.input_ids[0]) + max_new_tokens,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Stream tokens\n",
        "        response = \"\"\n",
        "        for token in output_text.split():  # Tokenize into words or subwords\n",
        "            response += token + \" \"\n",
        "            yield CompletionResponse(text=response.strip(), delta=token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "a05667713f0549bab26c7615b09ac2b7",
            "a8bd581047b64bcdaeeaf0de800267c6",
            "641c278577f747bca64661da39810a01",
            "f433577a2948489cb50cf8621350d2b1",
            "d4abbee0545f47d2878ec715f44d329a",
            "23f395215c504040b72acf792a757e98",
            "93c6bcfbe0e445f0891eb780e29d7198",
            "dab1b8e40cab4605a1b73028fe04d303",
            "0db852298f20446383a9e38533b86ab8",
            "01caad9afc884e5eb780e65a5bd81fb0",
            "ad111423808b4562b9d3960b74a2fc3d",
            "c3c865919fb649e8bac4d8db32d34185",
            "d1a7fe4670e34deab71d0f7b739713ec",
            "a93537140c6d41cd8db90b3f4cd68427",
            "6263f5b0b8b0485cb7f8d177ebcb6652",
            "4cb97d0a2d9845b5ae478649763fc1b9",
            "6c59a66c45204438b27fac3fe2e05f01",
            "c87eca9b671c4c00a56f5991d3ff1671",
            "1befe5050af547faa1986c498c2d3cbf",
            "af2b2fb41c5447bdac4ec142d6abdcee",
            "2c396933411842fb8b071e076cf8db98",
            "9a03c90784e343b59393d740d979e844",
            "cd7fee62020b4416abafa8a764a68797",
            "e38c951972414d828274d3c03b939f31",
            "d47dba22159e4d27b8b50d170a9c7ce7",
            "b332ad13ca984a81ab4839f567f17b2d",
            "15debb87424d4bb8ba8f39adb45003ef",
            "36e51701444f49e2bf96abcdd5274929",
            "5abc9faadaf5490382de53ca80eba2b8",
            "b1d80976052e4df5b5e806bf957ff348",
            "be45542de4a74fcf9e8ae301a6fd8624",
            "95f003a2d62b4f498272ba4614d47074",
            "3c32812264b84b3c80f506bcb1dab48e"
          ]
        },
        "id": "-XGCPloyFtw-",
        "outputId": "8ed38fd6-4eb4-4220-a8ad-d6c5a7998f9f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a05667713f0549bab26c7615b09ac2b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3c865919fb649e8bac4d8db32d34185",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd7fee62020b4416abafa8a764a68797",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "custom_llm = ArXivLLM(model=model, tokenizer=tokenizer, num_output=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz5rLgBPGgL4"
      },
      "outputs": [],
      "source": [
        "Settings.llm = custom_llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1sSyk-eGpTw",
        "outputId": "cf2c731b-f495-4673-bbab-8767e85ef690"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=128) and `max_length`(=1137) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context information is below.\n",
            "---------------------\n",
            "['page_label: 1\\nTitle of this paper: Real-World Single Image Super-Resolution Under Rainy Condition\\nAuthors: Mohammad Shahab Uddin\\nDate published: 06/16/2022\\nURL: http://arxiv.org/abs/2206.08345v1\\n\\nReal-World Single Image Super-Resolution Under \\nRainy Condition \\n \\nMohammad Shahab Uddin  \\nDepartment of Electrical and Computer Engineering \\nOld Dominion University  \\nNorfolk, USA \\nmuddi003@odu.edu\\nAbstract— Image super -resolution is an important research \\narea in computer vision that has a wide variety of applications \\nincluding surveillance, medical imaging etc. Real -world signal \\nimage super-resolution has become very popular now -a-days due \\nto its real -time application. There are still a lot of scopes to \\nimprove real-world single image super-resolution specially during \\nchallenging weather scenarios. In this paper, we have proposed a \\nnew algorithm to perform real -world single image super -\\nresolution during rainy condition.  Our proposed method can \\nmitigate the influence of rainy conditions during image super -\\nresolution. Our experiment results  show that our proposed \\nalgorithm can perform image super -resolution decreasing the \\nnegative effects of the rain. \\nKeywords—image super-resolution, rainy condition , deep \\nlearning \\nI. INTRODUCTION  \\nImage super -resolution denotes the enhancement of the \\nresolution of images from low resolution to high resolution. \\nImage super -resolution is extremely important for improving \\nperformance of deep learning models during several situations \\nincluding surveillance. Using super-resolution, we can improve \\nthe resolution of an image to get better classification and \\ndetection accuracy. Many methods are proposed for image \\n \\n  \\n  \\nFig. 1. Images under rainy condition.\\npage_label: 12\\nTitle of this paper: Lossless Image Compression through Super-Resolution\\nAuthors: Sheng Cao, Chao-Yuan Wu, Philipp Krähenbühl\\nDate published: 04/06/2020\\nURL: http://arxiv.org/abs/2004.02872v1\\n\\n12 Sheng Cao, Chao-Yuan Wu, Philipp Kr¨ ahenb¨ uhl\\nx(0) (full) x(0) x(3) sample 1 sample 2 sample 3 sample 4 sample 5\\nFig. 5. Super-resolution distribution visualization.We sample images from our\\nnetwork to visualize what it learns. From left to right: original imagex(0), x(0) (zoomed\\nin), downsampled image x(3), and samples. We sample images from full x(3), but just\\npresent the zoomed-in 1002-pixel view for clear presentation. (Best viewed on screen.)\\nCompression Rate. Table 2b shows detailed analysis of compression rate with\\nAC. We see that while x(3) is simply stored as raw images, it contributes to only\\na small fraction of the overall bpsp. Finer-level of super-resolution requires more\\nbpsp as expected, as ﬁner-grained details are harder to predict. Rounding bits\\nalso have less structure to exploit. We store them uncompressed, taking ∼24% of\\nthe overall bpsp. Metadata to store width and height is negligible. The rounding\\nin AC causes 0.01 bpsp increase compared to raw log-likelihoods.\\nScalability. Table 2c presents the runtime with AC when scaling to high-\\nresolution images. Images≥6402 come from the higher-resolution DIV2K dataset [3].\\nWe see that even for high-resolution 9602 images, both the encoding and decod-\\ning time stay practical. Encoding is more eﬃcient than decoding, because it\\nrequires fewer CPU-GPU synchronizations.\\n6.2 Qualitative Analysis\\nSuper-Resolution Distribution Visualization. Our network learns a dis-\\ntribution over possible super-resolutions. In Fig. 6, we present images sampled']\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: How is super resolution helpful?\n",
            "Answer: (1) Super resolution helps in improving the classification and detection accuracy of deep learning models. (2) Super resolution can mitigate the influence of rainy conditions during image super resolution. (3) Super resolution can decrease the negative effects of the rain on image super resolution.\n",
            "\n",
            "References:\n",
            "\n",
            "1. [1]: \"Real-World Single Image Super-Resolution Under Rainy Condition\", Mohammad Shahab Uddin, Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, USA, muddi003@odu.edu, arxiv.org/abs/2206.08345v1\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "from llama_index.core.retrievers import AutoMergingRetriever\n",
        "\n",
        "# configure retriever\n",
        "base_retriever = index.as_retriever(llm=custom_llm, similarity_top_k = 2)\n",
        "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
        "\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "            structured_answer_filtering=True,\n",
        "            response_mode=ResponseMode.SIMPLE_SUMMARIZE,\n",
        "        )\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever, response_synthesizer=response_synthesizer, llm = custom_llm)\n",
        "response= query_engine.query(\"\"\"How is super resolution helpful?\"\"\")\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "TPAHTA77UdzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  ragas datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOtwZdn_UdMj",
        "outputId": "4e2c9ee4-4d20-4f1f-ca52-477635c1475e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ragas\n",
            "Downloading ragas-0.2.0-py3-none-any.whl\n",
            "Collecting datasets\n",
            "Downloading datasets-2.19.1-py3-none-any.whl\n",
            "Installing collected packages: ragas, datasets\n",
            "Successfully installed ragas-0.2.0 datasets-2.19.1 ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFSPvLFvNe-W"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import faithfulness\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "query = \"How is super resolution helpful?\"\n",
        "\n",
        "# Generated answer\n",
        "response = query_engine.query(query)\n",
        "generated_answer = response.response\n",
        "\n",
        "\n",
        "retrieved_nodes = retriever.retrieve(query)\n",
        "contexts = [node.node.get_content() for node in retrieved_nodes]\n",
        "\n",
        "# Building dataset for RAGAS\n",
        "data = {\n",
        "    \"question\": [query],\n",
        "    \"answer\": [generated_answer],\n",
        "    \"contexts\": [contexts]\n",
        "}\n",
        "\n",
        "dataset = Dataset.from_dict(data)"
      ],
      "metadata": {
        "id": "pqqsGqF9UlQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate(\n",
        "    dataset,\n",
        "    metrics=[faithfulness],\n",
        ")\n",
        "\n",
        "print(\"Faithfulness Score:\",result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oodnxLTDUxG6",
        "outputId": "980471dc-0b94-4b33-c07f-e7a1c128b2ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating: 100%|████████████████████| 1/1 [00:03<00:00,  3.12s/it]\n",
            "\n",
            "Faithfulness Score: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kafCP2vsW_GQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}